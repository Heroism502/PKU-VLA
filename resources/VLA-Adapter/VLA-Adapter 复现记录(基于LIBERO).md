# VLA-Adapter å¤ç°è®°å½•ï¼ˆåŸºäº LIBEROï¼‰

## ğŸ’¡è¯´æ˜
![framework](framework.png)

VLA-Adapter å®ç°æœºå™¨äººæ§åˆ¶çš„å…³é”®æ€æƒ³æ˜¯ï¼š
åœ¨ä¸å»æ”¹åŠ¨/é‡è®­å¤§å°ºå¯¸è§†è§‰-è¯­è¨€æ¨¡å‹ä¸»ä½“çš„å‰æä¸‹ï¼Œç”¨ä¸€ä¸ªå¾ˆè–„çš„æ¡¥æ¥/é€‚é…æ¨¡å—ï¼ˆAdapter + Bridge Attentionï¼‰æŠŠâ€œå›¾åƒï¼‹æŒ‡ä»¤â€å¾—åˆ°çš„é€šç”¨è¯­ä¹‰ç‰¹å¾ï¼Œè½¬æˆä¸“é—¨ç»™åŠ¨ä½œé¢„æµ‹ç”¨çš„æœºå™¨äººè¡¨å¾ï¼Œè¿™æ ·æ—¢èƒ½ä¿ç•™å¤§æ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼Œåˆèƒ½æŠŠè®­ç»ƒé‡æ”¶æ•›åœ¨å¾ˆå°çš„ä¸€å—å‚æ•°ä¸Šã€‚

å…·ä½“æ¥è¯´ï¼š
è¾“å…¥æ˜¯ä¸€æ®µè§†è§‰è§‚æµ‹ï¼ˆå•å¸§/å¤šå¸§å›¾åƒï¼‰å’Œä¸€æ¡è¯­è¨€æŒ‡ä»¤ï¼Œå…ˆèµ°è¿›å·²ç»è®­ç»ƒå¥½çš„ VLM / LLM ä¸»å¹²ï¼ˆé€šå¸¸æ˜¯å†»ç»“çš„ï¼‰ï¼Œå¾—åˆ°ä¸€ä¸²è¯­ä¹‰+è§†è§‰æ··åˆçš„ token è¡¨ç¤ºã€‚
è¿™äº›è¡¨ç¤ºå¹¶ä¸èƒ½ç›´æ¥æ‹¿å»å‡ºæœºå™¨äººåŠ¨ä½œï¼Œå› æ­¤ VLA-Adapter åœ¨ä¸»å¹²åé¢æ’å…¥ä¸€ä¸ªè½»é‡çš„é€‚é…å±‚ï¼šå®ƒåšä¸¤ä»¶äº‹
ç”¨ Bridge / Cross-Attention æŠŠâ€œä»»åŠ¡ç›¸å…³çš„åŠ¨ä½œæŸ¥è¯¢ï¼ˆaction queryï¼‰â€å»å¯¹é½ä¸Šæ¸¸çš„è§†è§‰-è¯­è¨€ç‰¹å¾ï¼›
ç”¨é—¨æ§/æ®‹å·®çš„æ–¹å¼ï¼Œåªä¿ç•™å¯¹å½“å‰ä»»åŠ¡çœŸçš„æœ‰ç”¨çš„é‚£éƒ¨åˆ†ä¿¡æ¯ï¼Œé¿å…æŠŠå¤§æ¨¡å‹çš„å™ªå£°ä¼ åˆ°æ§åˆ¶ç«¯ã€‚

å’Œ OpenVLA æŠŠåŠ¨ä½œç¦»æ•£æˆ 256 ä¸ª LLM è¯è¡¨ token ä¸åŒçš„æ˜¯ï¼ŒVLA-Adapter ä¸€èˆ¬ä¸å»â€œå ç”¨â€å¤§æ¨¡å‹è¯è¡¨ï¼Œè€Œæ˜¯èµ°ä¸€æ¡å•ç‹¬çš„åŠ¨ä½œå¤´ï¼šæŠŠé€‚é…åçš„ç‰¹å¾é€è¿›ä¸€ä¸ªå°çš„ Transformer/MLP åŠ¨ä½œé¢„æµ‹å¤´ï¼Œç›´æ¥å›å½’/åˆ†ç±»å‡ºæœºå™¨äººçš„ä½ç»´æ§åˆ¶é‡ï¼ˆä¾‹å¦‚ 6/7 ç»´ä½å§¿å¢é‡ + å¤¹çˆªï¼‰ã€‚


> **ğŸ“ Paper: https://arxiv.org/abs/2509.09372**<br/>
> **ğŸŒ Project page: https://vla-adapter.github.io/**<br/>
> **ğŸ¤— HuggingFace: https://huggingface.co/VLA-Adapter**<br/>
> **Github: https://github.com/OpenHelix-Team/VLA-Adapter**



## 1. ç®€ä»‹

æœ¬ä»“åº“ç”¨äºè®°å½•åœ¨å¹¿å·¥æœåŠ¡å™¨ä¸Šå¤ç° **VLA-Adapter**ï¼ˆLIBERO ä»»åŠ¡ï¼‰å…¨è¿‡ç¨‹ï¼ŒåŒ…å«ï¼š

- ç¯å¢ƒæ­å»ºï¼ˆconda + ç¦»çº¿ pip + mujoco/robosuiteï¼‰
- æ•°æ®é›†è·¯å¾„é…ç½®ï¼ˆæœ¬åœ° `modified_libero_rlds`ï¼‰
- è®­ç»ƒè„šæœ¬ä¸å…³é”®å¯åŠ¨å‚æ•°
- æ¨ç† / è¯„ä¼°è„šæœ¬
- å¸¸è§æŠ¥é”™ä¸ä¿®å¤åŠæ³•ï¼ˆæœ¬æ¬¡å¤ç°çœŸå®å‘ç”Ÿè¿‡çš„ï¼‰
- 
## 2. ç¯å¢ƒæ­å»º

### 2.1å®‰è£…miniconda
å¹¿å·¥çš„æœåŠ¡å™¨é‡Œé¢æ²¡æœ‰minicondaï¼Œé¦–å…ˆè¦ä»å®‰è£…minicondaå¼€å§‹

```bash
cd ~
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py310_23.3.1-0-Linux-x86_64.sh
chmod +x Miniconda3-py310_23.3.1-0-Linux-x86_64.sh
bash Miniconda3-py310_23.3.1-0-Linux-x86_64.sh
```

å®‰è£…å®Œæ‰§è¡Œï¼š

```bash
source ~/.bashrc
conda --version
```

### 2.2 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
conda create -n adapter python=3.10 -y
conda activate adapter
```
### 2.3 å®‰è£…ä¾èµ–
```bash
pip install "torch==2.2.0" "torchvision==0.17.0" "torchaudio==2.2.0" \
    -i https://pypi.tuna.tsinghua.edu.cn/simple
```
å®‰è£…å®Œå¿«é€Ÿè·‘è‡ªæ£€
```bash
python - <<'PY'
import torch, torchvision, torchaudio
print("torch:", torch.__version__, "cuda:", torch.version.cuda, "available:", torch.cuda.is_available())
print("vision:", torchvision.__version__, "audio:", torchaudio.__version__)
if torch.cuda.is_available():
    print("device:", torch.cuda.get_device_name(0))
PY
```
æ‹‰å–æºç ,å®‰è£…ä¾èµ–
```bash
git clone https://bgithub.xyz/OpenHelix-Team/VLA-Adapter.git #ä½¿ç”¨é•œåƒç½‘ç«™
cd VLA-Adapter
pip install -e . #å¦‚æœæ— æ³•è®¿é—®githubï¼Œè¿›å…¥pyproject.tomlæŠŠå¯¹åº”github.comæ”¹æˆbgithub.com
pip install packaging ninja
ninja --version; echo $?  # Verify Ninja --> should return exit code "0"
```
å®‰è£…flash-attnï¼Œç”±äºæ— æ³•è®¿é—®githubï¼Œä½¿ç”¨ç¦»çº¿å®‰è£…
```bash
cd /home/caohaonian/wheels
wget "https://bgithub.xyz/Dao-AILab/flash-attention/releases/download/v2.5.5/flash_attn-2.5.5+cu122torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl" 
# å®‰è£…
pip install --no-index --find-links . \
  flash_attn-2.5.5+cu122torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
```
å¿«é€Ÿè‡ªæ£€,å®‰è£…æˆåŠŸä¼šæ˜¾ç¤ºç‰ˆæœ¬ï¼Œå¦‚æœé”™è¯¯è¯´æ˜C++ ABIï¼ˆcxx11abiï¼‰/ç¼–è¯‘å…¼å®¹æ€§ä¸åŒ¹é…ï¼Œå»ºè®®é—®aié€‰ä»€ä¹ˆç‰ˆæœ¬
```bash
python - <<'PY'
import importlib, torch
print("torch:", torch.__version__, "cuda:", torch.version.cuda)
m = importlib.util.find_spec("flash_attn_2_cuda") or importlib.util.find_spec("flash_attn_cuda")
print("flash_attn CUDA ext found:", bool(m))
import flash_attn
print("flash_attn:", getattr(flash_attn, "__version__", "unknown"))
PY
```
åˆ°è¿™ä¸€æ­¥ä¼šæœ‰ä¸€ä¸ªç‰ˆæœ¬å†²çªï¼Œæ²¡æœ‰å°±è·³è¿‡
```bash
tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.6 which is incompatible.
````
è§£å†³åŠæ³•:æ›¿æ¢ç‰ˆæœ¬ï¼Œå“ªä¸ªèƒ½æˆåŠŸç”¨å“ªä¸ª
```bash
pip uninstall -y numpy scipy
pip install -- "numpy==1.26.4" "scipy==1.11.4" 
````
### 2.4 å®‰è£…LIBERO
```bash
cd
git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git
pip install -e LIBERO
cd VLA_Adapter
pip install -r experiments/robot/libero/libero_requirements.txt
```
## 3. å¼€å§‹è®­ç»ƒ
### 3.1 ä¸‹è½½æ•°æ®é›†,ä¸‹è½½no_noopsçš„liberoæ•°æ®é›†ä¼šæ¯”è¾ƒå¥½
```bash
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download --repo-type dataset \
                         --resume-download \
                         --local-dir modified_libero_rlds \
                         openvla/modified_libero_rlds
```
### 3.2å¾®è°ƒlibero_objectä»»åŠ¡
```bash
mkdir -p /home/caohaonian/VLA-Adapter/logs
export NCCL_P2P_DISABLE=1
export NCCL_IB_DISABLE=1
export WANDB_MODE=offline
export TRANSFORMERS_OFFLINE=1
export HF_HUB_OFFLINE=1
CUDA_VISIBLE_DEVICES=0,1 MASTER_ADDR=127.0.0.1 MASTER_PORT=29501 \
WANDB_MODE=offline NCCL_P2P_DISABLE=1 NCCL_IB_DISABLE=1 \
torchrun --standalone --nproc-per-node 1 /home/caohaonian/VLA-Adapter/vla-scripts/finetune.py
--vlm_path /home/caohaonian/VLA-Adapter/pretrained_models/prism-qwen25-extra-dinosiglip-224px-0_5b
--config_file_path /home/caohaonian/VLA-Adapter/pretrained_models/configs
--data_root_dir /home/caohaonian/datasets/modified_libero_rlds
--dataset_name libero_object_no_noops
--run_root_dir /home/caohaonian/VLA-Adapter/outputs
--use_pro_version True
--use_lora True
--lora_rank 64
--use_film False
--use_minivlm True
--num_images_in_input 2
--use_proprio True
--image_aug True
--batch_size 1
--grad_accumulation_steps 8 --learning_rate 2e-4
--num_steps_before_decay 9000 --max_steps 10000
--save_freq 1000 --save_latest_checkpoint_only False
--merge_lora_during_training True
--wandb_entity ""
--wandb_project ""   2>&1 | tee -a "$LOG"

```
### 3.3å¾®è°ƒlibero_longä»»åŠ¡
å¾®è°ƒlongä»»åŠ¡ä¸èƒ½æŠŠæ˜¾å­˜ç”¨å¤ªå¤šï¼Œéœ€è¦ç•™ä¸€ç‚¹ç»™merge_lora_during_training True
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 MASTER_ADDR=127.0.0.1 MASTER_PORT=29501 \
WANDB_MODE=offline NCCL_P2P_DISABLE=1 NCCL_IB_DISABLE=1 \
torchrun --standalone --nproc-per-node 4 /home/caohaonian/VLA-Adapter/vla-scripts/finetune.py \
  --vlm_path /home/caohaonian/VLA-Adapter/pretrained_models/prism-qwen25-extra-dinosiglip-224px-0_5b \
  --config_file_path /home/caohaonian/VLA-Adapter/pretrained_models/configs \
  --data_root_dir /home/caohaonian/datasets/modified_libero_rlds \
  --dataset_name libero_10_no_noops \
  --run_root_dir /home/caohaonian/VLA-Adapter/outputs \
  --use_pro_version True --use_lora True --lora_rank 64 \
  --use_film False --use_minivlm True --num_images_in_input 2 \
  --use_proprio True --image_aug True \
  --batch_size 4 \
  --grad_accumulation_steps 8 \
  --learning_rate 2e-4 \
  --num_steps_before_decay 45000 \
  --max_steps 50000 \
  --save_freq 5000 \
  --save_latest_checkpoint_only False \
  --merge_lora_during_training True \
  --wandb_entity "" --wandb_project "" \
  2>&1 | tee -a /home/caohaonian/VLA-Adapter/logs/libero_long_bs4x8_$(date +%Y%m%d_%H%M%S).log
```
### 3.4å…¶ä»–liberoä»»åŠ¡

## 4.å®éªŒç»“æœ
éƒ¨åˆ†ç»“æœå¦‚å›¾ï¼š
![liberoå®éªŒ](liberoå®éªŒ.png)
