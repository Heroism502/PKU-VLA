# PKU-VLA

## 2025

- **Title:** Unified Vision-Language-Action Model (UniVLA)  
  **Authors:** Yuqi Wang (CASIA, BAAI)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2506.19850](https://arxiv.org/abs/2506.19850)  
  **Code:** [GitHub - baaivision/UniVLA](https://github.com/baaivision/UniVLA)

- **Title:** MEMORYVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation  
  **Authors:** Hao Shi (Tsinghua University)  
  **Venue:** arXiv 2025
  **Paper:** [Project Page](https://shihao1895.github.io/MemoryVLA/)  
  **Code:** [GitHub - shihao1895/MemoryVLA](https://shihao1895.github.io/MemoryVLA/)

- **Title:** SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning  
  **Authors:** Haozhan Li (Tsinghua University)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2509.09674](https://arxiv.org/abs/2509.09674)  
  **Code:** [GitHub - PRIME-RL/SimpleVLA-RL](https://github.com/PRIME-RL/SimpleVLA-RL)

- **Title:** Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks  
  **Authors:** Wenqi Zhang (Zhejiang University)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2503.21696](https://arxiv.org/abs/2503.21696)  
  **Code:** [GitHub - zwq2018/embodied_reasoner](https://github.com/zwq2018/embodied_reasoner)

- **Title:** Galaxea Open-World Dataset and G0 Dual-System VLA Model  
  **Authors:** Galaxea Collaboration  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2509.00576](https://arxiv.org/abs/2509.00576)  
  **Code:** [GitHub - OpenGalaxea/G0](https://github.com/OpenGalaxea/G0)

- **Title:** SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics  
  **Authors:** Mustafa Shukor (Hugging Face)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2506.01844](https://arxiv.org/abs/2506.01844)  
  **Code:** [GitHub - huggingface/lerobot](https://github.com/huggingface/lerobot/tree/main/src/lerobot/policies/smolvla)

- **Title:** SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model  
  **Authors:** Delin Qu (Shanghai AI Laboratory)  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2501.15830](https://arxiv.org/abs/2501.15830)  
  **Code:** [GitHub - SpatialVLA/SpatialVLA](https://github.com/SpatialVLA/SpatialVLA)

- **Title:** InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning  
  **Authors:** Ji Zhang(University of Electronic Science and Technology of China)  
  **Venue:** arXiv 2025-05
  **Paper:** [arXiv:2505.13888](https://arxiv.org/abs/2505.13888)
  **Code:** [GitHub - Koorye/Inspire](https://github.com/Koorye/Inspire)

- **Title:** VLA-RL:Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning
  **Authors:** Guanxing Lu(Tsinghua University)
  **Venue:** arXiv 2025-05
  **Paper:** [arXiv:2505.18719](https://arxiv.org/abs/2505.18719)
  **Code:** [GitHub - GuanxingLu/vlar](https://github.com/GuanxingLu/vlarl?tab=readme-ov-file)

- **Title:** Interactive Post-Training for Vision-Language-Action Models
  **Authors:** Shuhan Tan(UT Austin, Nankai University)
  **Venue:** CVPR 2025(arXiv 2025-05)
  **Paper:** [arXiv:2505.17016](https://arxiv.org/abs/2505.17016)
  **Code:** [GitHub - Ariostgx/ript-vla](https://github.com/Ariostgx/ript-vla)

- **Title:** What Can RL Bring to VLA Generalization？An Empirical Study
  **Authors:** Jijia Liu(Tsinghua University)
  **Venue:** arXiv 2025-05
  **Paper:** [arXiv:2505.19789](https://arxiv.org/abs/2505.19789)
  **Code:** [GitHub - gen-robot/RL4VLA](https://github.com/gen-robot/RL4VLA)

- **Title:** BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation
  **Authors:** Hongyu Wang(Key Laboratory of AI Safety)
  **Venue:** arXiv 2025-06
  **Paper:** [arXiv:2506.07530](https://arxiv.org/abs/2506.07530)
  **Code:** [GitHub - ustcwhy/BitVLA](https://github.com/ustcwhy/BitVLA?tab=readme-ov-file)

- **Title:** VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model
  **Authors:** Yihao Wang(Beijing University of Posts and Telecommunications ,Westlake University ,Zhejiang University,OpenHelix Team)
  **Venue:** arXiv 2025-09
  **Paper:** [arXiv:2509.09372](https://arxiv.org/abs/2509.09372)
  **Code:** [GitHub - OpenHelix-Team/VLA-Adapter](https://github.com/OpenHelix-Team/VLA-Adapter)

- **Title:** NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks
  **Authors:** Chia-Yu Hung(Singapore University of Technology and Design,Lambda Labs)
  **Venue:** arXiv 2025-04
  **Paper:** [arXiv:2504.19854](https://arxiv.org/abs/2504.19854)
  **Code:** [GitHub - declare-lab/nora](https://github.com/declare-lab/nora)

- **Title:** ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model
  **Authors:** Zhongyi Zhou(Midea Group)
  **Venue:** arXiv 2025-02
  **Paper:** [arXiv:2502.14420](https://arxiv.org/abs/2502.14420)
  **Code:** [GitHub - tutujingyugang1/ChatVLA](https://github.com/tutujingyugang1/ChatVLA_public?tab=readme-ov-file)


## 2024

- **Title:** OpenVLA: An Open-Source Vision-Language-Action Model  
  **Authors:** Moo Jin Kim (Stanford University)  
  **Venue:** CoRL 2024  
  **Paper:** [arXiv:2406.09246](https://arxiv.org/abs/2406.09246)  
  **Code:** [GitHub - openvla/openvla](https://github.com/openvla/openvla)

- **Title:** π0: A Vision-Language-Action Flow Model for General Robot Control  
  **Authors:** Kevin Black (Physical Intelligence Company)  
  **Venue:** arXiv 2024 
  **Paper:** [arXiv:2410.24164](https://arxiv.org/abs/2410.24164)  
  **Code:** [GitHub - Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi)

- **Title:** CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models  
  **Authors:** Haoxu Huang (Tsinghua University)  
  **Venue:** IROS 2024  
  **Paper:** [arXiv:2403.08248](https://arxiv.org/abs/2403.08248)  
  **Code:** [GitHub - HaoxuHuang/copa](https://github.com/HaoxuHuang/copa)


- **Title:** TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation  
  **Authors:** Junjie Wen (Midea Group)  
  **Venue:** arXiv 2024
  **Paper:** [arXiv:2409.12514](https://arxiv.org/abs/2409.12514)  
  **Code:** [GitHub - liyaxuanliyaxuan/TinyVLA](https://github.com/liyaxuanliyaxuan/TinyVLA)

- **Title:** CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation
  **Authors:** Qixiu Li(Tsinghua University)  
  **Venue:** arXiv 2024-11
  **Paper:** [arXiv:2411.19650](https://arxiv.org/abs/2411.19650)  
  **Code:** [GitHub - microsoft/CogACT](https://github.com/microsoft/CogACT?tab=readme-ov-file)


## 2023

- **Title:** VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models  
  **Authors:** Wenlong Huang (Stanford University)  
  **Venue:** CoRL 2023 
  **Paper:** [arXiv:2307.05973](https://arxiv.org/abs/2307.05973)  
  **Code:** [GitHub - huangwl18/VoxPoser](https://github.com/huangwl18/VoxPoser)

- **Title:** A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter  
  **Authors:** Kechun Xu 
  **Venue:** ICRA 2023(arXiv 2023-02)
  **Paper:** [arXiv:2302.12610](https://arxiv.org/abs/2302.12610)
  **Code:** [GitHub - xukechun/Vision-Language-Grasping](https://github.com/xukechun/Vision-Language-Grasping)
