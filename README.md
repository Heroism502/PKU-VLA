# PKU-VLA

## 2025

- **Title:** Unified Vision-Language-Action Model (UniVLA)  
  **Authors:** Y. Wang, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2506.19850](https://arxiv.org/abs/2506.19850)  
  **Code:** [GitHub](https://github.com/baaivision/UniVLA)  :contentReference[oaicite:0]{index=0}

- **Title:** Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks  
  **Authors:** W. Zhang, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2503.21696](https://arxiv.org/abs/2503.21696)  
  **Code:** [Project Page](https://embodied-reasoner.github.io/)  :contentReference[oaicite:1]{index=1}

- **Title:** SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics  
  **Authors:** M. Shukor, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2506.01844](https://arxiv.org/abs/2506.01844)  
  **Code:** —  :contentReference[oaicite:2]{index=2}

- **Title:** SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning  
  **Authors:** H. Li, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2509.09674](https://arxiv.org/abs/2509.09674)  
  **Code:** [GitHub](https://github.com/PRIME-RL/SimpleVLA-RL)  :contentReference[oaicite:3]{index=3}

- **Title:** SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model  
  **Authors:** D. Qu, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2501.15830](https://arxiv.org/abs/2501.15830)  
  **Code:** [GitHub](https://github.com/SpatialVLA/SpatialVLA) · [Site](https://spatialvla.github.io)  :contentReference[oaicite:4]{index=4}

- **Title:** Galaxea Open-World Dataset and G0 Dual-System VLA Model  
  **Authors:** T. Jiang, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2509.00576](https://arxiv.org/abs/2509.00576)  
  **Code:** [G0 on GitHub](https://github.com/OpenGalaxea/G0) · [Project](https://opengalaxea.github.io/G0/)  :contentReference[oaicite:5]{index=5}

- **Title:** MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation  
  **Authors:** H. Shi, et al.  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2508.19236](https://arxiv.org/abs/2508.19236)  
  **Code:** —  :contentReference[oaicite:6]{index=6}


## 2024

- **Title:** π0: A Vision-Language-Action Flow Model for General Robot Control  
  **Authors:** K. Black, et al.  
  **Venue:** arXiv 2024  
  **Paper:** [arXiv:2410.24164](https://arxiv.org/abs/2410.24164)  
  **Code:** —  :contentReference[oaicite:7]{index=7}

- **Title:** TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation  
  **Authors:** J. Wen, et al.  
  **Venue:** arXiv 2024  
  **Paper:** [arXiv:2409.12514](https://arxiv.org/abs/2409.12514)  
  **Code:** [Site](https://tiny-vla.github.io)  :contentReference[oaicite:8]{index=8}

- **Title:** OpenVLA: An Open-Source Vision-Language-Action Model  
  **Authors:** M. J. Kim, et al.  
  **Venue:** arXiv 2024  
  **Paper:** [arXiv:2406.09246](https://arxiv.org/abs/2406.09246)  
  **Code:** [GitHub](https://github.com/openvla/openvla) · [Project](https://openvla.github.io)  :contentReference[oaicite:9]{index=9}

- **Title:** CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models  
  **Authors:** H. Huang, et al.  
  **Venue:** arXiv 2024  
  **Paper:** [arXiv:2403.08248](https://arxiv.org/abs/2403.08248)  
  **Code:** [Project Page](https://copa-2024.github.io)  :contentReference[oaicite:10]{index=10}


## 2023

- **Title:** VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models  
  **Authors:** W. Huang, et al.  
  **Venue:** arXiv 2023  
  **Paper:** [arXiv:2307.05973](https://arxiv.org/abs/2307.05973)  
  **Code:** [Project](https://voxposer.github.io/) · [PDF](https://voxposer.github.io/voxposer.pdf)  :contentReference[oaicite:11]{index=11}
