# PKU-VLA

## 2025

- **Title:** Unified Vision-Language-Action Model (UniVLA)  
  **Authors:** Yuqi Wang (CASIA, BAAI)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2506.19850](https://arxiv.org/abs/2506.19850)  
  **Code:** [GitHub - baaivision/UniVLA](https://github.com/baaivision/UniVLA)

- **Title:** MEMORYVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation  
  **Authors:** Hao Shi (Tsinghua University)  
  **Venue:** arXiv 2025
  **Paper:** [Project Page](https://shihao1895.github.io/MemoryVLA/)  
  **Code:** [GitHub - shihao1895/MemoryVLA](https://shihao1895.github.io/MemoryVLA/)

- **Title:** SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning  
  **Authors:** Haozhan Li (Tsinghua University)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2509.09674](https://arxiv.org/abs/2509.09674)  
  **Code:** [GitHub - PRIME-RL/SimpleVLA-RL](https://github.com/PRIME-RL/SimpleVLA-RL)

- **Title:** Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks  
  **Authors:** Wenqi Zhang (Zhejiang University)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2503.21696](https://arxiv.org/abs/2503.21696)  
  **Code:** [GitHub - zwq2018/embodied_reasoner](https://github.com/zwq2018/embodied_reasoner)

- **Title:** Galaxea Open-World Dataset and G0 Dual-System VLA Model  
  **Authors:** Galaxea Collaboration  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2509.00576](https://arxiv.org/abs/2509.00576)  
  **Code:** [GitHub - OpenGalaxea/G0](https://github.com/OpenGalaxea/G0)

- **Title:** SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics  
  **Authors:** Mustafa Shukor (Hugging Face)  
  **Venue:** arXiv 2025
  **Paper:** [arXiv:2506.01844](https://arxiv.org/abs/2506.01844)  
  **Code:** [GitHub - huggingface/lerobot](https://github.com/huggingface/lerobot/tree/main/src/lerobot/policies/smolvla)

- **Title:** SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model  
  **Authors:** Delin Qu (Shanghai AI Laboratory)  
  **Venue:** arXiv 2025  
  **Paper:** [arXiv:2501.15830](https://arxiv.org/abs/2501.15830)  
  **Code:** [GitHub - SpatialVLA/SpatialVLA](https://github.com/SpatialVLA/SpatialVLA)

## 2024

- **Title:** OpenVLA: An Open-Source Vision-Language-Action Model  
  **Authors:** Moo Jin Kim (Stanford University)  
  **Venue:** CoRL 2024  
  **Paper:** [arXiv:2406.09246](https://arxiv.org/abs/2406.09246)  
  **Code:** [GitHub - openvla/openvla](https://github.com/openvla/openvla)

- **Title:** Ï€0: A Vision-Language-Action Flow Model for General Robot Control  
  **Authors:** Kevin Black (Physical Intelligence Company)  
  **Venue:** arXiv 2024 
  **Paper:** [arXiv:2410.24164](https://arxiv.org/abs/2410.24164)  
  **Code:** [GitHub - Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi)

- **Title:** CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models  
  **Authors:** Haoxu Huang (Tsinghua University)  
  **Venue:** IROS 2024  
  **Paper:** [arXiv:2403.08248](https://arxiv.org/abs/2403.08248)  
  **Code:** [GitHub - HaoxuHuang/copa](https://github.com/HaoxuHuang/copa)


- **Title:** TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation  
  **Authors:** Junjie Wen (Midea Group)  
  **Venue:** arXiv 2024
  **Paper:** [arXiv:2409.12514](https://arxiv.org/abs/2409.12514)  
  **Code:** [GitHub - liyaxuanliyaxuan/TinyVLA](https://github.com/liyaxuanliyaxuan/TinyVLA)


## 2023

- **Title:** VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models  
  **Authors:** Wenlong Huang (Stanford University)  
  **Venue:** CoRL 2023 
  **Paper:** [arXiv:2307.05973](https://arxiv.org/abs/2307.05973)  
  **Code:** [GitHub - huangwl18/VoxPoser](https://github.com/huangwl18/VoxPoser)
